# 目次
- [前提知識](#前提知識)
- [履歴にまつわるアンチパターン](#履歴にまつわるアンチパターン)


# 前提知識
| 制約の種類 | 説明 |
|:-----------|:------------|
| PRIMARY KEY制約 | 重複とNULLがなく、そのテーブルで一意な行であることが確定される |
| NOT NULL制約 | NULLがないことが確定 |
| UNIQUE制約 | その値がテーブルで一意であることを確定される(NULLは許容) |
| CHECK制約 | 指定した条件の値のみが保存されていることを確定される |
| DEFAULT制約 | 値が指定されない時に保存される値を決める |
| FOREIGN KEY制約 | 別テーブルの主キーと参照整合性が保たれていることを確定させる |

CHECK制約 → 「ageカラムは0以上」のような条件をつけて、それをバリデーションに値を弾くやつ

## アンチパターンを生まないためには？
**「動くものを作る時に適切に作る」**

**「わかりづらい設計やな名前はデータベースの破綻の始まり」**

具体的には、
- 命名ミスは初期段階で対処
- 今後を想定した命名

一般的に **「DBの寿命はアプリケーションの寿命より長い」** → DBの技術的負債はより早く返済する必要がある

## リファクタリングの例
| 順番 | 例 |
|:-----------| :- |
| ① 変更後の名前のカラムを新規カラムとして生成 |
| ② ①で作ったカラムは、トリガーを使って変更前のデータを同じにする | 古いカラムへのinsertやupdateをトリガーにする
| ③ サービス単位やモデル単位で、参照or更新した際に新しい方のカラムに設定し直す |
| ④ 切り替えが完了したタイミングで、古いカラムはdropする |

↑(感想) 知らなかった。長期で使っていく前提でのやり方っぽいからかな

***

<!-- TODO: ここから第2章 -->
# 履歴にまつわるアンチパターン

## どんなアンチパターン??
### ❌ 履歴データから、どの操作を行ったかがわからない
  - ex) 売上データと商品のマスタデータの単価が合わない...

### ❌ 過去の "事実" が失われる
  - ex) 商品の単価が変わった時に、過去のデータを上書きしてしまう...
  - → **商品名や価格が変わると売り上げの事実と不整合が生じる**

### ❌ 過去の "過程" が見つけにくい
  - ex) 注文のステータスを変更する...

## ベストプラクティス
⭕️ **履歴データを残す**
  - ex) 消費税の変更を例にすると...
    - 消費税のレコードに、有効期限を設ける
    - 売り上げのテーブルに消費税の値を持たせる
  - ex) 注文のステータスでは...
    - ステータスが変更される際は、新しくレコードを追加し、最新のレコードを有効なレコードとして扱う
  - ex) 金融系のシステムでは、削除処理も「打ち消しのINSERT」として保存して、合計値を算出することがよくある

履歴データをDBにないに残さず、ログで管理する方法もある！！ (cloud watchに残して障害時はそこを見るなど)

### 😓 デメリット
- データ量が増えやすい
  - → 検索スピードが落ちる
  - → テーブルサイズが増える(金もかかる)

## 設計時の観点・ポイント
- 非常時に欲しいデータが残っているか？？
- データの変化を追えているか？？

## 感想
トレードオフになるので、(時にスピードは如実にUXに関わるので)、設計段階でドメインエキスパートと非常時に必要なデータはどれで、どこまで捨てるか、拾うかのすり合わせが大事そう。
ログ残すのは大体やってると思うから、アプリケーション側で履歴を追うような要件(ユーザー側での履歴確認など)がなかったら、残さなくても良さそう？？

***

# JOINにまつわるアンチパターン
🚨 **JOINはパフォーマンスに直結するので扱いに注意**

## JOINの種類とイメージ図
![INNER JOIN](/image/database/unti_pattern/left_outher_join.jpg)
![そのほかのJOIN](/image/database/unti_pattern/other_join.jpg)

よく使うのは、だいたい`LEFT OUTER JOIN`

<details>
<summary><h2>JOINの仕組み(難しかった😭)</h2></summary>

JOINを高速化するには、JOINする際に用いるアルゴリズムを知る必要がある
### `Nested Loop Join(NLJ)`
![NLJ](/image/database/unti_pattern/nested_loop_join.jpg)
### `Hash Join`
![hashJoin](/image/database/unti_pattern/hash_join.png)
### `Sort Merge Join`
![sortMergeJoin](/image/database/unti_pattern/sort_merge_join.png)

## 各種アルゴリズムの特徴
![algorithmFeature](/image/database/unti_pattern/algorithm_feature.jpg)
</details>

## RDBMSによるJOINの違い
- MySQL
  - `NLJ`のみサポート
- PostgreSQL
  - 上記3種類のすべてをサポートしているので、MySQLより不等号や大きな2つの表のJOINが得意(詳しくは `JOINの仕組み` の`各種アルゴリズムの特徴` を参照)

**MySQLはより、INDEXを活用した設計が求められる**

## どんなアンチパターン？？
### ❌ JOINしすぎ
- JOINするテーブルが多いと、パフォーマンスが悪くなる
- パフォーマンスが悪くなる理由は、JOIN先が増えるたび、ベン図の重なる部分が指数関数的に増えるから(計算量も指数関数的に増加)
  - ex) 3つのテーブルをJOIN → 3パターン(3C2 = 3)
  - ex) 4つのテーブルをJOIN → 6パターン(4C2 = 6)

## ベストプラクティス
### ⭕️ 無駄にJOINしない
### ⭕️ JOINするテーブルは小さくしてからJOINする(JOIN前に絞り込みをしてデータ量を減らす)
### ⭕️ INDEXを活用する
- INDEXを活用することで、データが大きくなった際は、SortMergeJoinが効く

## 感想
JUJU、データ量多くて、一覧画面でスロークエリになってたから、JOINする箇所とかタイミング、見直した方が良いかも。

***

# INDEXにまつわるアンチパターン
## どんなアンチパターン？？
### ❌ INDEXを張っているが、INDEXが効いていない
- where句でindexが指定されていない
  - ex) age にindexを貼っているとする
    - INDEXが効く例 : `SELECT * FROM users WHERE age > 20`
    - INDEXが効かない例 : `SELECT * FROM users WHERE age * 2 > 40`
    - **※ 同じ条件でも、後者ではindexではなく、あくまでも `age * 2`という計算結果を条件にしているため、indexが効かない**
- 検索結果が多い
- 全体の件数が少ない
- カーディナリティが低いカラムに対する検索
- あいまいな検索(LIKE)
- 統計情報と実際にテーブルで乖離がある

## INDEXの役割
- INDEXは、その名の通り、索引の役割を果たす
- イメージとしては、図書館でジャンル別に並べられているから本が探しやすくなるイメージ(だいぶ効率的✨)

ex) 以下は`WHERE user_id = 4000`というクエリがあった場合、50倍も早くなるよの図
- indexを使うと1つのデータを取り出すために、4ブロックにアクセスしている = 100行取り出すには400 I/O

  <img src="../image/database/unti_pattern/btree_index.jpg" width="500">
  <img src="../image/database/unti_pattern/full_scan.jpg" width="500">
  <img src="../image/database/unti_pattern/use_index.jpg" width="500">

## INDEXが効く条件
1. **検索結果がテーブル全体の20%以下**
    - できるなら、10%未満が良い
2. **検索対象のテーブルが十分に大きい**
    - 数万 ~ 数十万レコードが目安
    - 1000行程度では、テーブルスキャンの方が効率が良いケースが多い

## INDEXを貼る際の考慮事項
- テーブルのサイズが数年後、どれくらいになるか？ を見積もる
- INDEXが複合INDEXでまとめられる or 単一のINDEXで十分ではないか？？
  - インデックスショットガンを避けるため
- 今、このINDEXを貼るべきか？？
  - 新規サービスなど、データ傾向が見えない場合は、様子見をしてから判断した方が良い

## ベストプラクティス
### ⭕️ where句でindexが指定する
  - ex) age にindexを貼っているとする
    - INDEXが効く例 : `SELECT * FROM users WHERE age > 20`
    - INDEXが効かない例 : `SELECT * FROM users WHERE age * 2 > 40`
    - **※ 同じ条件でも、後者ではindexではなく、あくまでも `age * 2`という計算結果を条件にしているため、indexが効かない**
### ⭕️ INDEXを張るカラムは、カーディナリティが高いカラムを選ぶ
  - カーディナリティが高い = 重複が少ないデータ
  - ex) booleanだと、0 or 1しかないので、カーディナリティが高い(かぶっているデータが多い)
    - これだと、データ量が少なくならないので、indexの意味がなくなる
### ⭕️ なるべくLIKE検索を避ける
  - LIKE検索は、前方一致以外はindexが効かない
  - ex) `SELECT * FROM users WHERE name LIKE '%山田%'`
    - この場合、indexが効かないので、全件検索になる
### ⭕️ 統計情報と日データの乖離をなくす
  - 統計情報が古いと、最適なindexが選ばれない
    - indexを使うかはオプティマイザと呼ばれるものが統計情報を元に決めている
    - この統計情報は定期的にサンプリングしたデータを元に作成されている
    - オプティマイザは、あくまで「統計情報」をもとに判断するので、未来のデータは見れない。そこも含めてどうテーブルを設計するかが、腕の見せ所
  - STGではindexが使われていたが、本番では使われていないということがある場合はこのケースが多い
### ⭕️ INDEXを設定しすぎない
  - あまりにもindexを張りすぎると、データの更新が遅くなる
    - オプティマイザが適切なINDEXを判断できなくなるため
    - 「インデックスショットガン」と言われている
  - indexを張ると、データの更新時にindexも更新されるため、データの更新が遅くなる
  - なので、indexを張るかどうかは、データの更新頻度によって変わる

## Tips
### MENTORの法則
| MENTORの法則 | 説明 |
|:-----------|:------------|
| M: Measure(測定)    | スロークエリやDBのパフォーマンスなどをモニタリング |
| E: Explain(解析)    | 実行計画を見て、クエリが遅くなっている原因を追求 |
| N: Nominate(指名)   | ボトルネックの原因を特定(インデックスが未定義など) |
| T: Test(試験)       | ボトルネックの改善を実施し、処理時間を測定。改善後の全体的なパフォーマンスを確認 |
| O: Optimize(最適化) | DBパラメータの最適化を定期的に実施し、インデックスがキャッシュメモリに載るように最適化 |
| R: Rebuild(再構築)  | 統計情報やインデックスを定期的に再構築 |

***

# フラグにまつわるアンチパターン
## どんなアンチパターン？？
### ❌ フラグが多い
- クエリが複雑になる
  - JOINするたびにwhere句が増える
- UNIQUE制約が効かない
- カーディナリティが低い
  - カーディナリティが低い = 重複が多いデータ(フラグは0 or 1しかないので、カーディナリティが低い)
  - 特に削除フラグの場合、ほとんどの検索で使われるためボトルネックになりがち

原因は **「テーブルに状態を持たせたこと」**
フラグの問題は、規模が大きくなってから顕在化しやすい

## ベストプラクティス
### ⭕️ 事実のみを保存する
- 会員テーブルがあるとすると、会員が退会した場合は、別の削除済み会員テーブルにデータを作成して、会員テーブルのデータは削除する
- こうすることで、会員テーブルから削除フラグがなくなり、テーブルが状態を持たなくなる
- このような機能はアプリ側ではなくDB側の「トリガー」という機能で設定できる
  - ただし、トリガーは仕様変更に弱いので注意が必要

## テーブルが状態を保つことを許容しても良いケース
- 対象テーブルが小さく、INDEXが不要
- そのテーブルが親テーブルになることがない or 少ない (JOINされる回数が減るので)
- UNIQUE制約が不要で、外部キーでデータの生合成を担保する必要がない

## 削除フラグを使っても良いユースケース
- ユーザーから見えなくしたいが、データは消したくない
- 削除データを検索したい
- 操作を誤っても無かったことにしたい
- 削除してもすぐに元に戻したい

***

# ソートにまつわるアンチパターン
## ソートの仕組み
RDBは、リレーショナルモデルという考え方を元にしており、以下の3つの特徴がある。
- 重複がない
- 実在する要素しかない(NULLがない)
- **順序がない** <--- 今回はここがポイント

RDBはリレーショナルモデルにない考えを扱うのが苦手なので `ORDER BY` の処理も苦手。

RDBでは、以下の実行順でSQLを評価している。(※`ORDER BY`は最後の方に評価されている)

<img src="../image/database/unti_pattern/rdbms_executer.jpeg" width="500">

## ベストプラクティス
### ⭕️ ORDER BY句を狙ったINDEX
- ORDER BY句にINDEXを貼ったカラムを指定すると、ソート処理が高速化される
  - INDEXを貼っていないカラムの場合
    - テーブルスキャンが発生する(全件取得してからソートする)
  - INDEXを貼っているカラムの場合
    - INDEXでソート済みのデータから取得するため、高速に取得できる
      - → ソートが不要になる
      - → 評価数がLIMITの件数に達した時点で結果を返せる

### データを小さくする
- 絞り込んだデータが小さいほど、高速になる
- **ページネーションの実装で「次のページ」として渡すのは、OFFSETの値より最後に表示されたIDにした方が早くなる**
  - **OFFSETはINDEXが貼られていなく、IDには貼られているから**

## Tips
Railsは遅延評価と言って、ActiveRecordのオブジェクト生成時にはSQLは発行されず、実際にデータが必要になったタイミングで、最適なSQLが発行される
つまり、`limit()`や`where()`の順番は違ってもActiveRecord側でいい感じに最適化してくれる

### 参考資料
- [SmartHR: Active Recordともっと仲良くなって自然に優しいコードを書くぞ](https://tech.smarthr.jp/entry/2021/11/11/151444)
- [Zenn: 【Rails】ActiveRecordはどのタイミングで実際にクエリを実行しているのか。SQLキャッシュについて。](https://zenn.dev/nakamura_fumiya/articles/c3ebcbe66a70b6)

## 感想
> ページネーションの実装で「次のページ」として渡すのは、OFFSETの値より最後に表示されたIDにした方が早くなる

これは知らなかった。今までのPJではどれもOFFSETの値を元にしていた(気がする)ので、次回からはIDにしてみようと思う。

RedisなどのNoSQLもほとんど触ったことないから、そういったところも勉強していきたい。

***

# 隠された状態
## どんなアンチパターン？？
### ❌ 意味を含んだID
- ただの数値であるIDに意味を持たせているパターン
  - ex) 管理者のIDが`1`から始まるなど...
### ❌ EAV(エンティティ・アトリビュート・バリュー)
- カラムを増やさずに、属性に複数の目的の情報を持たせるパターン
  - 実際にで値を取り出すまで何のデータかわからない = 何のデータかは取り出した値にしか書いてないから

  <img src="../image/database/unti_pattern/eav.jpeg" width="500">

### ❌ Polymorphic Associations(ポリモーフィック関連)
- 子テーブルが複数の親テーブルを持つような設計
  - 参照先によて親テーブルが変わるため、外部キー制約が貼れない...
  - Railsでは`ActionStorage`などで採用しているため、一概にアンチパターンとは言えない？？🤔
  - 書籍的には **「EAはレコード単位で、PolymorphicAssociationsはテーブル単位での状態を隠している** という考えらしい

いづれにしても、**「制約が張りづらい = 不正なデータが入りやすい」** という大きなデメリットを持つ
隠された状態はアプリケーション側にも影響が大きいアンチパターン(enumなども状態を隠している)
## ベストプラクティス
### ⭕️ データにビジネスロジックを持たせない
### ⭕️ テーブルを複数の目的に使わない
- PolymorphicAssociationの場合は、テーブルごとに中間テーブルを作成することで対応できる

## 感想
アプリケーションのコードと変わらず、基本は単一責任の原則なんだなと思った。

***

# なんでもJSON
JSONのようなスキーマレスはRDBと相性が悪い(EAVと同様の理由)

## どんなアンチパターン？？
### ❌ 検索が複雑になる(SQLが使いづらくなる)
- SQLでもJSONの中身を検索できるが、SQLが複雑になって検索コストが上がる
### ❌ 更新コストが上がる
- 上と同様の理由
### ❌ ORMが使えない
- 多くのORMはJSON型に対応していないため、複雑なSQLを直接書く必要がある
- 取り入れる際は、ORMのサポートなどJSON型との相性を考慮する必要がある
- ActiveRecordでは、`json`型が使える
### ❌ データの整合性が取りづらい
- EAVと同じくRDBの制約が効かないため、データの整合性が取りづらい

## ベストプラクティス
### ⭕️ JSON型を使うケースを限定する
- WebAPIのレスポンスなど、そもそもがJSONで返ってくる場合に使う
  - APIは予告なく変更される場合があるので、スキーマを定義すると返り値がエラーになるケースもあり得るため、JSONが向いてる
- OS情報など、データによって微妙な差分が生じる時
  - OSのディストリビューションによって固有のデータがある場合もあるので、そういった差分の情報をうまく吸収したいケースにはJSONが適している
  - また、更新もほとんどないので、データの整合性が取りづらいという問題も発生しづらい
  - JSONのkeyで検索したいケースがある場合は必須項目などを選定してスキーマに落とした方が良い

JSON型は、**「RDBの機能と引き換えに柔軟性を与える、最後の切り札」** として使うのが良い

***

# 強すぎる制約
## どんなアンチパターン？？
### ❌ 早すぎる最適化のせいで、仕様変更に弱い
- 早すぎる段階で制約を設けすぎることで、仕様変更に弱くなる
  - 早い段階で独自型を使ってバリデーションしているとその傾向にある
### ❌ 外部キー制約によるデッドロック
- MySQLでは、子テーブルの更新時に親テーブルのデータまでロックするため、デッドロックが発生しやすい
  - PostgreSQLでは、デッドロックが発生しづらい
### ❌ ENUM型を使う
- ENUM型は独自型と同じようなデメリットを持っているため。同じく仕様変更に弱い
- 利用側がENUMで定義した値を知っていないと追加、更新できない
### ❌ 状態を持つCHECK制約
- ex) INSERT時は、updated_atが現在の時刻以上であるという制約を入れる
  - これだとテストデータやバックアップ用データのリストア時にエラーが起きる

## ベストプラクティス
### ⭕️ アプリ側でもバリデーションをかける
- **DB側での制約とアプリ側でのバリデーションは、両方使うことが大切！！**
- 強めな制約はアプリケーション側でかけた方が良い
  - DBの制約とアプリのバリデーションでは、DBの制約を変更する方がコストが高いため
### ⭕️ タイミングに応じて制約を強める
- データは成長する生き物なので、データの変化に応じて適切なタイミングで制約をかけることが大事
- **変更されそうなビジネスロジックは、DBの制約には入れない**

  <img src="../image/database/unti_pattern/db_lock.jpeg" width="500">

***

# バックアップにまつわるアンチパターン
## バックアップの種類とその違い

<img src="../image/database/unti_pattern/backup.jpeg" width="500">
<details>
<summary>各種バックアップの特徴</summary>

### 論理バックアップ
- SQLやCSVとして、DBそのものを再構築できるバックアップ
  #### [ メリット ]
  - 中身はテキストなので閲覧も可能
  - 他のRDBMSなどに移植しやすい
  #### [ デメリット ]
  - ファイルサイズが大きくなりがち
  - リストアに時間がかかる
  - バックアップ時点に戻すだけなので、それ以降のデータは消える

### 物理バックアップ
- DBの物理ファイルを丸ごとバックアップする手法
  #### [ メリット ]
  - 最小限のサイズで取得できる
  - リストアが速い
  - 復旧方法も楽
    - ファイルを置き換えて再起動するだけ
    - ※ ただし、復旧するには一度DBを停止する必要がある
  #### [ デメリット ]
  - 移植性が低い
    - 他のRDBMSやバージョンごとの互換性がない場合が多い
    - バックアップ時点に戻すだけなので、それ以降のデータは消える

### ポイントインタイムリカバリ(PITR)
- 特定の日時の状態にデータをリストアできる手法
- リストアするには、バックアップファイルと更新情報の入ったログが必要
  #### [ メリット ]
  - 障害が起きた直前の状態に戻せる
  #### [ デメリット ]
  - サイズが大きくなる
    - バックアップファイルとログの両方が必要なため
  - 復旧の手順が難しい
</details>

## バックアップの設計
以下の3つの観点をもとに、バックアップの設計を行う
全てをバックアップするにはコストが大きいので、サービスごとにどこまで許容できるかなどのバランスが求められる
- **復旧できるデータ** (RPO: Recovery Point Objective)
- **復旧までの時間** (RTO: Recovery Time Objective)
- **復旧したいデータ**(RLO: Recovery Level Objective)

これらに加えて、稼働率という考え方も大事。

以下は、稼働率とそれに対応する年間停止時間と必要な対策
| 稼働率    | 年間停止時間 | 難易度 |
|:---------|:-----------|:------|
| 90%      | 36.5日     | バックアップとリストアで十分 |
| 99%      | 3.65日     | オンプレでは予備マシンが必要。大規模データならリストア所用時間の把握が必要 |
| 99.9%    | 8.76時間   | 法定停電への対応や24/365のサポートなど、システム外の仕組みづくりも必要 |
| 99.99%   | 52.56分    | バックアップからのリストアだけでは難しい。コールドスタンバイなどが必要 |
| 99.999%  | 5.26分     | 遅延レプリケーションなどの専用システムが必要 |
| 99.9999% | 31.5秒     | 無停止サーバーが必要。コストは非常に高い |

## どんなアンチパターン？？
### ❌ バックアップの失敗に気づけない
### ❌ バックアップの復旧ができない

## ベストプラクティス
### ⭕️ バックアップ失敗に気づく仕組みを作る
- 失敗時に通知が飛ぶ仕組みづくり
  - ディスク容量が急激に変化した場合も要注意!!

### ⭕️ バックアップ、リストアの自動化

### ⭕️ チーム内で定期的にリストアする機会を設ける
- 手順書などドキュメントに残しておく
- 定期的に復旧テストを行う
- これらの属人化しない仕組みを作る

### ⭕️ クラウドサービスを利用する
- RDSでは1回/日のフルバックアップと、5分に1回のログのバックアップをとっており、PITRも楽にできる
